{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an example of how to use pretrained language model, as well as how to extract the text representation from the pretrained model. You can use google colab or kaggle for free GPU resource. \n",
    "\n",
    "kaggle: https://www.kaggle.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (4.21.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andreas\\.julia\\conda\\3\\envs\\citation\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "# you can use this command to install libraries you don't have\n",
    "#!pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if GPU is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/Andreas/Downloads/dataset/training_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Graph cuts optimization was applied to the output of a hierarchical two-stage classifier, which was trained to identify the cartilage and bone voxels MAINCIT . As the underlying multi-label graph cuts jointly consider independent classifier outputs for cartilage, bone, and background, label-conflict-resolution may be challenging in regions with multiple labels.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_df.citation_context.values[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer and pretrained language model\n",
    "You can find more pretrained language models on: https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tokenizer to get the token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10629,  7659, 20600,  2001,  4162,  2000,  1996,  6434,  1997,\n",
       "          1037, 25835,  2048,  1011,  2754,  2465, 18095,  1010,  2029,  2001,\n",
       "          4738,  2000,  6709,  1996, 11122, 11733,  3351,  1998,  5923, 29450,\n",
       "          9050,  2364, 26243,  1012,  2004,  1996, 10318,  4800,  1011,  3830,\n",
       "         10629,  7659, 10776,  5136,  2981,  2465, 18095, 27852,  2005, 11122,\n",
       "         11733,  3351,  1010,  5923,  1010,  1998,  4281,  1010,  3830,  1011,\n",
       "          4736,  1011,  5813,  2089,  2022, 10368,  1999,  4655,  2007,  3674,\n",
       "         10873,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input = tokenizer(example, return_tensors = 'pt', max_length = 512, truncation = True, padding = 'max_length').to(DEVICE)\n",
    "example_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the text representation from DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5604, -0.2248, -0.3066,  ..., -0.2467, -0.0082,  0.4634],\n",
       "         [-0.1042, -0.3635, -0.2408,  ...,  0.3454,  0.4058,  0.6786],\n",
       "         [ 0.0271, -0.1428, -0.1220,  ...,  0.0729, -0.0081, -0.0783],\n",
       "         ...,\n",
       "         [-0.2101, -0.1302,  0.1094,  ..., -0.2197, -0.1336,  0.1365],\n",
       "         [ 0.0850,  0.1100,  0.0469,  ..., -0.4255, -0.2457,  0.1477],\n",
       "         [-0.3461,  0.0067,  0.0949,  ..., -0.2213, -0.0230, -0.0052]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)\n",
    "example_representation = model(**example_input).last_hidden_state\n",
    "example_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_representation.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_representation_mean = example_representation.mean(dim = 1)\n",
    "example_representation_mean.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading paper from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]  Semantic Context Forests for Learning-Based Knee Cartilage Segmentation in 3D MR Images Learning-Based Knee Cartilage Segmentation Quan WangFORMULA   [SEP]',\n",
       " '[CLS] Dijia WuFORMULA   [SEP]',\n",
       " '[CLS] Le LuFORMULA   [SEP]',\n",
       " '[CLS] Meizhu Liu FORMULA Kim L. BoyerFORMULA   [SEP]',\n",
       " '[CLS] Shaohua Kevin ZhouFORMULA  Q. Wang D. Wu [SEP]',\n",
       " '[CLS] L. Lu M. Liu K. L. Boyer S. K. Zhou FORMULA Siemens Corporate Research, Princeton, NJ 08540, USA FORMULA Rensselaer Polytechnic Institute, Troy, NY 12180, USA Learning-Based Knee Cartilage Segmentation Quan Wang, et al. The automatic segmentation of human knee cartilage from 3D MR images is a useful yet challenging task due to the thin sheet structure of the cartilage with diffuse boundaries and inhomogeneous intensities. [SEP]',\n",
       " '[CLS] In this paper, we present an iterative multi-class learning method to segment the femoral, tibial and patellar cartilage simultaneously, which effectively exploits the spatial contextual constraints between bone and cartilage, and also between different cartilages. [SEP]',\n",
       " '[CLS] First, based on the fact that the cartilage grows in only certain area of the corresponding bone surface, we extract the distance features of not only to the surface of the bone, but more informatively, to the densely registered anatomical landmarks on the bone surface. [SEP]',\n",
       " '[CLS] Second, we introduce a set of iterative discriminative classifiers that at each iteration, probability comparison features are constructed from the class confidence maps derived by previously learned classifiers. [SEP]',\n",
       " '[CLS] These features automatically embed the semantic context information between different cartilages of interest. [SEP]',\n",
       " '[CLS] Validated on a total of 176 volumes from the Osteoarthritis Initiative (OAI) dataset, the proposed approach demonstrates high robustness and accuracy of segmentation in comparison with existing state-of-the-art MR cartilage segmentation methods.   [SEP]',\n",
       " '[CLS] Introduction   [SEP]',\n",
       " '[CLS] The quantitative analysis of knee cartilage is advantageous for the study of cartilage morphology and physiology. [SEP]',\n",
       " '[CLS] In particular, it is an important prerequisite for the clinical assessment and surgical planning of the cartilage diseases, such as knee osteoarthritis which is characterized as the cartilage deterioration and a prevalent cause of disability among elderly population. [SEP]',\n",
       " '[CLS] As the leading imaging modality used for articular cartilage quantification {{cite:e88f7616-a421-4044-8330-6ef8249623a3}}, magnetic resonance (MR) imaging provides direct and noninvasive visualization of the whole knee joint including the soft cartilage tissues (Fig. REF ). [SEP]',\n",
       " '[CLS] However, automatic segmentation of the cartilage tissues from MR images, which is required for accurate and reproducible quantitative cartilage measures, still remains an open problem because of the inhomogeneity, small size, low tissue contrast, and shape irregularity of the cartilage. [SEP]',\n",
       " \"[CLS] An earlier endeavor on this problem is Folkesson et al.'s voxel classification approach {{cite:f462207d-348a-4f5b-acb2-75d6e785bdb7}}, which runs an approximate FORMULA NN classifier on voxel intensity and absolute position based features. [SEP]\",\n",
       " '[CLS] However, due to the overlap of intensity distribution between cartilage and other tissues such as menisci and muscles, as well as the variability of the cartilage locations from scan to scan, the performance of this method is limited. [SEP]',\n",
       " '[CLS] More recently, Vincent et al. have developed a knee joint segmentation approach based on active appearance model (AAM), which captures the statistics of both object shape and image cues. [SEP]',\n",
       " '[CLS] Though promising results are reported in {{cite:1f693ff6-9fc8-434d-8af1-61d32eacd085}}, the search for the initial model pose parameter can be very time consuming even if a coarse to fine searching strategy is used. [SEP]',\n",
       " '[CLS] Given the strong spatial relation between the cartilages and bones in the knee joint, most proposed cartilage segmentation methods are based on a framework that each bone is segmented first in the knee joint {{cite:e4961cb0-17cd-4f12-80cd-cfd22d934925}}, {{cite:b907ee07-bf9f-44e2-901c-e393de0fc7ab}}, {{cite:44b9f5d5-6ad4-4010-ac74-d17804f8a0fa}}, which is usually easier than direct cartilage segmentation because the bones are much larger in size with more regular shapes. [SEP]',\n",
       " '[CLS] Fripp et al. segment the bones based on 3D active shape model (ASM) incorporating the cartilage thickness statistics, and the outer cartilage boundary is then determined by examining the intensity profile along the normal to the bone surface, while being constrained by the cartilage thickness model {{cite:e4961cb0-17cd-4f12-80cd-cfd22d934925}}. [SEP]',\n",
       " \"[CLS] In Yin's work {{cite:b907ee07-bf9f-44e2-901c-e393de0fc7ab}}, the volume of interest containing the bones and cartilages is first detected using a learning-based approach, then the bones and cartilages are jointly segmented by solving an optimal multi-surface detection problem via multi-column graph cuts {{cite:83344ecf-07bf-4152-a7e9-a5b2ed4dffd6}}. [SEP]\",\n",
       " '[CLS] Lee et al. employ a constrained branch-and-mincut method with shape priors to obtain the bone surface, and then segment the cartilage with MRF optimization based on local shape and appearance information {{cite:44b9f5d5-6ad4-4010-ac74-d17804f8a0fa}}. [SEP]',\n",
       " '[CLS] In spite of the differences, these approaches all require classification of bone surface voxels into bone cartilage interface (BCI) and non-BCI, which is an important intermediate step to determine the search space or impose prior constraint for cartilage segmentation. [SEP]',\n",
       " '[CLS] Therefore, any classification error of BCI will probably propagate to the final cartilage segmentation result. [SEP]',\n",
       " '[CLS] In this paper, we present a fully automatic learning-based voxel classification method for cartilage segmentation. [SEP]',\n",
       " '[CLS] It also requires pre-segmentation of corresponding bones in the knee joint. [SEP]',\n",
       " '[CLS] However, the new approach does not rely on explicit classification of BCI. [SEP]',\n",
       " '[CLS] Instead, we construct distance features from each voxel to a large number of anatomical landmarks on the surface of the bones to capture the spatial relation between the cartilages and bones. [SEP]',\n",
       " '[CLS] By removing the intermediate step of BCI extraction, the whole framework is simplified and classification error propagation can be avoided. [SEP]',\n",
       " '[CLS] Besides the connection between the cartilages and bones, strong spatial relation also exists among different cartilages which is more often overlooked in earlier approaches. [SEP]',\n",
       " '[CLS] For example, the femoral cartilage is always above the tibial cartilage and two cartilages touch each other in the region where two bones slide over each other during joint movements. [SEP]',\n",
       " '[CLS] To utilize this constraint, we introduce the iterative discriminative classification that at each iteration, the multi-class probability maps obtained by previous classifiers are used to extract semantic context features. [SEP]',\n",
       " '[CLS] In particular, we compare the probabilities at positions with random shift and compute the difference. [SEP]',\n",
       " '[CLS] These features, which we name as the random shift probability difference (RSPD) features, are more computationally efficient and more flexible for different range of context compared to the calculation of probability statistics at fixed relative positions {{cite:d7e4dcf4-2147-4e52-8230-0453d7dfad25}}, {{cite:907c2fe4-9c5a-4dd0-8b2f-0228d01399bd}}. [SEP]',\n",
       " '[CLS]   Review of Bone Segmentation    [SEP]',\n",
       " '[CLS] In this work, we employ a learning-based bone segmentation approach which has shown the efficiency and effectiveness in different medical image segmentation problems {{cite:004302cf-455c-4e22-bcd4-b83e8285ad7a}}, {{cite:0d642c4b-abb7-4636-a24b-7abc62733388}}. [SEP]',\n",
       " '[CLS] We represent the shape of a bone by a closed triangle mesh FORMULA . [SEP]',\n",
       " '[CLS] Given a number of training volumes with manual bone annotations, we use the coherent point drift algorithm (CPD) {{cite:9803d261-51bd-4395-a2b0-cf3e68a5a465}} to find anatomical correspondences of the mesh points and thereof construct the statistical shape models with mean shape FORMULA  {{cite:8f4aaad3-c040-4c3d-997d-b14d54bbb5bd}}. [SEP]',\n",
       " '[CLS] As shown in Fig. REF , the whole bone segmentation framework comprises three steps. [SEP]',\n",
       " '[CLS]   [SEP]',\n",
       " '[CLS] Pose Estimation: For a volume FORMULA , the bone is first localized by searching for the (sub-)optimal pose parameters FORMULA , i.e., the translation, rotation and anisotropic scaling, using the marginal space learning (MSL) {{cite:0d642c4b-abb7-4636-a24b-7abc62733388}}:  FORMULA  and the shape is initialized by linearly transforming the mean shape FORMULA . [SEP]',\n",
       " '[CLS]   [SEP]',\n",
       " '[CLS] Model Deformation: At this stage, the shape is repeatedly deformed to fit the boundary and projected to the variation subspace until convergence.   [SEP]',\n",
       " '[CLS] Boundary Refinement: To further improve the segmentation accuracy, we use the random walks algorithm {{cite:55f69e2b-9b39-44ed-97b3-8326c671deaf}} to refine the bone boundary (see Table REF  and Fig. REF  for results) and employ the CPD algorithm to obtain anatomically equivalent landmarks on the refined bone surface. [SEP]',\n",
       " '[CLS]    Cartilage Classification    [SEP]',\n",
       " '[CLS] Given all three knee bones being segmented, we first extract a band of interest within a maximum distance threshold from each of the bone surface, and only classify voxels in the band of interest to simplify the training and testing by removing irrelevant negative voxels. [SEP]',\n",
       " '[CLS] Feature Extraction For each voxel with spatial coordinate FORMULA , we construct a number of base features which can be categorized into three subsets. [SEP]',\n",
       " '[CLS] Intensity Features include the voxel intensity and its gradient magnitude, respectively: FORMULA , FORMULA . [SEP]',\n",
       " '[CLS] Distance Features measure the signed Euclidean distances from each voxel to different knee bone boundaries: FORMULA , FORMULA , FORMULA , where FORMULA  is the signed distance to the femur, FORMULA  to tibia, and FORMULA  to patella. [SEP]',\n",
       " '[CLS] Then we have their linear combinations:  FORMULA   [SEP]',\n",
       " '[CLS] These features are useful because the sum features FORMULA  and FORMULA  measure whether voxel FORMULA  locates within the narrow space between two bones, and the difference features FORMULA  and FORMULA  measure which bone it is closer to. [SEP]',\n",
       " '[CLS] Fig. [SEP]',\n",
       " '[CLS] REF  shows how FORMULA  and FORMULA  in addition to intensity feature FORMULA  separate tibial cartilage from femoral and patellar cartilages. [SEP]',\n",
       " '[CLS] Given the prior knowledge that the cartilage can only grow in certain area on the bone surface, it is useful for the cartilage segmentation to not only know how close the voxel is to the bone surface, but also where it is anatomically. [SEP]',\n",
       " '[CLS] Therefore we define the distance features to the densely registered landmarks on the bone surface as described in Section : FORMULA , where FORMULA  is the spatial coordinate of the FORMULA th landmark of all bone mesh points. [SEP]',\n",
       " '[CLS] FORMULA  is randomly generated in training due to the great number of mesh points available (Fig. REF ). [SEP]',\n",
       " '[CLS]  TABLE  FIGURE   FIGURE   FIGURE   [SEP]',\n",
       " '[CLS] Context Features compare the intensity of the current voxel FORMULA  and another voxel FORMULA  with random offset FORMULA : FORMULA , where FORMULA  is a random offset vector. [SEP]',\n",
       " '[CLS] This subset of features, named as random shift intensity difference (RSID) features in this paper, capture the context information in different ranges by randomly generating a large number of different values of FORMULA  from a uniform distribution in training. [SEP]',\n",
       " '[CLS] They were earlier used to solve pose classification {{cite:3da14ff6-2f17-4e79-ac37-01279df0e3ad}} and keypoint recognition {{cite:a52b98f5-f270-408d-ad8f-ea1b585a04a0}} problems. [SEP]',\n",
       " '[CLS]   [SEP]',\n",
       " '[CLS] Iterative Semantic Context Forests In this paper, we present a multi-pass iterative classification method to automatically exploit the semantic context for multiple object segmentation problems. [SEP]',\n",
       " '[CLS] In each pass, the generated probability maps will be used to extract the context embedded features to enhance the classification performance of the next pass. [SEP]',\n",
       " '[CLS] Fig. REF  shows a 2-pass iterative classification framework with the random forests {{cite:3da14ff6-2f17-4e79-ac37-01279df0e3ad}}, {{cite:a52b98f5-f270-408d-ad8f-ea1b585a04a0}}, {{cite:70fcaa52-1e0e-4e5c-99af-3ea7fc94e697}}, {{cite:edf3b8ca-cbf2-4c8b-ba78-38f9456b214a}}, {{cite:edb7c1aa-f3fc-4057-a826-4914eff90c23}}, {{cite:ecdb107c-4df6-4a21-bed9-811a9a6a911a}} selected as the base classifier for each pass. [SEP]',\n",
       " '[CLS] However, the method can be extended to more iterations with the use of other discriminative classifiers. [SEP]',\n",
       " '[CLS] Semantic Context Features After each pass of the classification, the probability maps are generated and used to extract semantic context features as defined below: FORMULA , FORMULA , FORMULA , where FORMULA , FORMULA  and FORMULA  stand for the femoral, tibia and patellar cartilage probability map, respectively. [SEP]',\n",
       " '[CLS] In the same fashion as the RSID features, we compare the probability response of two voxels with random shift:  FORMULA  which is called random shift probability difference features (RSPD). [SEP]',\n",
       " '[CLS] RSPD provides semantic context information because the probability map values are directly associated with anatomical labels, rather than original intensity volume. [SEP]',\n",
       " '[CLS] In Fig. REF , it can be observed that the probability map of the second pass classification is significantly enhanced with much less noisy responses, compared with the first pass.   [SEP]',\n",
       " '[CLS] Post-processing by Graph Cuts Optimization After the classification, we finally use the probabilities of being the background and the three cartilages to construct the energy functions and perform multi-label graph cuts {{cite:ee6554ca-9a54-4f9d-bb30-47ca8e0e4f02}} to refine the segmentation with smoothness constraints. [SEP]',\n",
       " '[CLS] The graph cuts algorithm assigns a label FORMULA  to each voxel FORMULA , such that the energy below is minimized: FORMULA  where FORMULA  is the global label configuration, FORMULA  is the neighborhood system, FORMULA  is the smoothness energy, and FORMULA  is the data energy. [SEP]',\n",
       " '[CLS] We define FORMULA  FORMULA  takes value 1 when FORMULA  and FORMULA  are different labels, and takes value 0 when FORMULA . [SEP]',\n",
       " '[CLS] FORMULA  takes the value FORMULA , FORMULA , FORMULA  or FORMULA , depending on the label FORMULA . [SEP]',\n",
       " '[CLS] FORMULA  and FORMULA  are two parameters. [SEP]',\n",
       " '[CLS] FORMULA  specifies the weight of data energy versus smoothness energy, while FORMULA  is associated with the image noise {{cite:12f59899-4726-4770-9e7d-ab1b2fc6bc65}}. [SEP]',\n",
       " '[CLS]   Experimental Results  Dataset and Experiment Settings [SEP]',\n",
       " '[CLS] The dataset we use in our work is the publicly available Osteoarthritis Initiative (OAI) dataset, which contains both 3D MR images and ground truth cartilage annotations, referred to as “kMRI segmentations (iMorphics)”. [SEP]',\n",
       " '[CLS] The sagittal 3D 3T (Tesla) DESS (dual echo steady state) WE (water-excitation) MR images in OAI have high-resolution, good delineation of articular cartilage, fast acquisition time and high SNR. [SEP]',\n",
       " '[CLS] Our dataset consists of 176 volumes from 88 subjects, and belongs to the Progression subcohort, where all subjects show symptoms of OA. [SEP]',\n",
       " '[CLS] Each subject has two volumes scanned in different years. [SEP]',\n",
       " '[CLS] The size of each image volume is FORMULA  voxels, and the voxel size is FORMULA . [SEP]',\n",
       " '[CLS] For the validation, we divide the OAI dataset to three equally-sized subsets: FORMULA , FORMULA  and FORMULA , and perform a three-fold validation. [SEP]',\n",
       " '[CLS] The two volumes from the same subject are always placed in the same subset. [SEP]',\n",
       " '[CLS] For each randomized decision tree, we set the depth of the tree to 18, and train 60 trees in each pass. [SEP]',\n",
       " '[CLS] During training, the number of candidates at each non-leaf node is set to 1000. [SEP]',\n",
       " '[CLS] The dice similarity coefficient (DSC) is used to measure the performance of our method since it is commonly reported in previous literature {{cite:f462207d-348a-4f5b-acb2-75d6e785bdb7}}, {{cite:e4961cb0-17cd-4f12-80cd-cfd22d934925}}, {{cite:b907ee07-bf9f-44e2-901c-e393de0fc7ab}}, {{cite:44b9f5d5-6ad4-4010-ac74-d17804f8a0fa}}, {{cite:a1a95dd7-c126-4d05-8b87-817cc48e12c4}}. [SEP]',\n",
       " '[CLS]   [SEP]',\n",
       " '[CLS] Results First, we compare the frequency of different features that is selected by the classifiers. [SEP]',\n",
       " '[CLS] As shown in Fig. REF , RSID, RSPD and the distance to dense landmarks are very informative features to embed spatial constraints. [SEP]',\n",
       " '[CLS] Then we compare the segmentation performance with and without the use of the distance features to the anatomical dense landmarks, and also the results with different number of classification iterations. [SEP]',\n",
       " '[CLS] The results in Fig. REF  demonstrate the effectiveness of the distance features to dense landmarks and iterative classification with semantic context forests. [SEP]',\n",
       " '[CLS] In particular, 2-pass random forests achieve significant performance improvement, whereas the gain seems quite negligible by adding more passes. [SEP]',\n",
       " '[CLS] Finally, the quantitative results (2-pass classification) are listed in Table REF  together with the numbers reported in the earlier literature. [SEP]',\n",
       " '[CLS] Because the datasets used are different by all these approaches, the numbers in the table are only for reference. [SEP]',\n",
       " '[CLS] Note that only our experiments are based on a relatively large dataset. [SEP]',\n",
       " '[CLS] As shown in the table, we achieved high performance with regard to the femoral and tibial cartilage, whereas the DSC of patellar cartilage is notably lower than the other two cartilages. [SEP]',\n",
       " '[CLS] This is partly because the size of patellar cartilage is much smaller than femoral and tibial cartilage, so that the same amount of segmentation error will result in lower DSC. [SEP]',\n",
       " '[CLS] Besides, some patellar cartilage annotations in the dataset do not appear very consistent with others. [SEP]',\n",
       " '[CLS] Example segmentation results are shown in Fig. REF . [SEP]',\n",
       " '[CLS] FIGURE   [SEP]',\n",
       " '[CLS] FIGURE    Conclusion   We have presented a new approach to segment the three knee cartilages in 3-D MR images, which effectively exploits the semantic context information in the knee joint. [SEP]',\n",
       " '[CLS] By using the distance features to the bone surface as well as to the dense anatomical landmarks on the bone surface, the spatial constraints between cartilages and bones are incorporated without the need of explicit extraction of the bone cartilage interface. [SEP]',\n",
       " '[CLS] Furthermore, the use of multi-pass iterative classification with semantic context forests provides more spatial constraints between different cartilages to further improve the segmentation. [SEP]',\n",
       " '[CLS] The experiment validation shows the effectiveness of this method. [SEP]',\n",
       " '[CLS] Ongoing work include the joint bone-and-cartilage voxel classification in the iterative classification framework. [SEP]',\n",
       " '[CLS] TABLE    [SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('C:/Users/Andreas/Downloads/dataset/papers/1307.2965.txt', encoding='utf-8') as file:\n",
    "    paper = file.readlines()\n",
    "paper = [line.strip() for line in paper]\n",
    "paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining text representation fo the whole paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_sum = torch.zeros(1, 512, 768).to(DEVICE)\n",
    "for sent in paper:\n",
    "    sent_input = tokenizer(sent, return_tensors = 'pt', max_length = 512, truncation = True, padding = 'max_length').to(DEVICE)\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        sent_sum  += model(**sent_input).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_mean = sent_sum.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating cosine similarity between the paper and citation context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = F.cosine_similarity(example_representation_mean, paper_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9062259793281555"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
